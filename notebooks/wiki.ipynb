{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Planet Four Wiki\n",
      "* [Data reduction](#Data reduction)\n",
      "* [Standard reduction pipeline](#Standard reduction pipeline)\n",
      "* [Gold standard checker tool](#Gold standard checker)\n",
      "\n",
      "## Data reduction <a id='Data reduction'></a>\n",
      "### Format\n",
      "The target format is the Hierarchical Data Format (HDF) in version 5, a well established data format\n",
      "with good reading routines for Python, Matlab and IDL.\n",
      "\n",
      "### Parsing\n",
      "The first step is just a straight forward parsing of the CSV output of the Mongo database dump.\n",
      "While parsing, values of 'null' are being replaced by numpy.NaN.\n",
      "I made the conscious decision to _NOT_ replace `None` in the `marking` column by NaN because that\n",
      "detail is in itself useable data.\n",
      "\n",
      "Both the `acquisition_date` and the `created_at` column are currently being parsed to a python datetime.\n",
      "This has been made optional by calling the reduction routine with the option `--raw_times`.\n",
      "\n",
      "### Filtering / Cleaning\n",
      "\n",
      "Some markings for fans and blotches have some of their required data fields empty. By default we are removing these from the HDF5 database files. The way this is done is:\n",
      "\n",
      "1. Define the required columns for both fan and blotch markings. These are:\n",
      "    \n",
      "```python\n",
      "blotch_data_cols = 'x y image_x image_y radius_1 radius_2'.split()\n",
      "fan_data_cols = 'x y image_x image_y distance angle spread'.split()\n",
      "```\n",
      "2. For each marking `['fan', 'blotch']` do:\n",
      "    \n",
      "    1. Split the data in this `marking` data and the `rest`.\n",
      "    2. Filter the `marking ` data for the respective required columns.\n",
      "    3. Filter out any rows that have any of these fields empty.\n",
      "    4. Combine the reduced `marking` data with the `rest`.\n",
      "\n",
      "**To be noted: These incomplete data are not only from the first days during the TV event, but, albeit at lower frequency, scattered throughout the next year.**\n",
      "\n",
      "### Application\n",
      "\n",
      "The application is called `reduction.py` and when called with `-h` for help, it provides the following output:\n",
      "\n",
      "```bash\n",
      "    usage: planet4_reduction.py [-h] [--raw_times] [--keep_dirt] csv_fname\n",
      "\n",
      "    positional arguments:\n",
      "      csv_fname    Provide the filename of the database dump csv-file here.\n",
      "\n",
      "    optional arguments:\n",
      "          -h, --help   show this help message and exit\n",
      "      --raw_times  Do not parse the times into a Python datetime object. For the\n",
      "               stone-age. ;) Default: parse into datetime object.\n",
      "      --keep_dirt  Do not filter for dirty data. Keep everything. Default: Do the\n",
      "                   filtering.\n",
      "```\n",
      "\n",
      "### Reduction levels\n",
      "I produce different versions of the reduced dataset, increasing in reduction, resulting in smaller\n",
      "and faster to read files.\n",
      "\n",
      "For all file names the date part indicates the date of the database dump which is delivered every\n",
      "by Stuart.\n",
      "\n",
      "#### Level `Fast_Read`\n",
      "This file is a fixed table format for all cleaned data,\n",
      "in case one needs to read everything into memory the fastest way.\n",
      "\n",
      "Above mentioned filtering was applied, so tutorials and incomplete data rows removed.\n",
      "\n",
      "Product file name is `yyyy-mm-dd_planet_four_classifications_fast_all_read.h5`\n",
      "\n",
      "#### Level `Queryable`\n",
      "This file is the data of Level `Fast_Read`, but combined with a multi-column index, to be able to query the database file.\n",
      "The data columns that can be filtered for are:\n",
      "\n",
      "    data_columns=['classification_id', 'image_id',\n",
      "              'image_name', 'user_name', 'marking',\n",
      "              'acquisition_date', 'local_mars_time'])\n",
      "The way querying works (amazingly fast, btw.), for example to get all data for one image_id:\n",
      "    data = pd.read_hdf(database_fname, 'df', where='image_id=<image_id>')\n",
      "where `df` is the HDF internal handle for the table. This is required because HDF files can contain more than one table structure.\n",
      "\n",
      "Product file name is `yyyy-mm-dd_planet_four_classifications_queryable.h5`\n",
      "\n",
      "#### Level `Retired` (not yet implemented in reduction.py)\n",
      "This product is reduced to only included image_id's that have been retired (> 30 analyses done.)\n",
      "\n",
      "Product file name is `yyyy-mm-dd_planet_four_classifications_retired.h5`\n",
      "\n",
      "## Standard reduction pipeline <a id='Standard reduction pipeline'></a>\n",
      "### Action summary (for the impatient)\n",
      "\n",
      "**Note: This procedure currently creates approx 5 GB of data on top of the downloaded file size.**\n",
      "\n",
      "* Download data dump `xxx.csv.tar.gz` from the email you get every Sunday (if not, contact Meg).\n",
      "* on Mac, the Archive utility should properly unpack it, on linux:\n",
      "    * `tar zxvf yyyy_mm_dd_xxxx.csv.tar.gz`\n",
      "* clone this repository. (see [here](https://github.com/CitizenScienceInAstronomyWorkshop/P4_sandbox/blob/master/README.md) how.)\n",
      "* `cd` into `P4_sandbox/planet4`\n",
      "* `python reduction.py path_to_csv_file` The argument is the full path to the unpacked CSV file.\n",
      "\n",
      "### Additional help\n",
      "\n",
      "You need a current Python environment that includes the following modules:\n",
      "    \n",
      "* pandas\n",
      "* PyTables (tables)\n",
      "* scipy / numpy\n",
      "\n",
      "I can recommend the [Anaconda distribution](https://store.continuum.io/cshop/anaconda/) from Continuum Analytics, it contains extra features for academic users. But I have also used [Enthought's Canopy](https://www.enthought.com/products/canopy/) successfully for years, just on Linux I don't like the hoops one has to go through for a multi-users installation.\n",
      "\n",
      "### What it does\n",
      "It will create both the queryable and fast-read HDF5 database files in the same folder where the given CSV file is stored.\n",
      "\n",
      "## Gold standard checker <a id='Gold standard checker'></a>\n",
      "\n",
      "I wrote a litte checking tool that enables you to see for each of the html gold standard link files that Meg sends around each week which of the entries you have done already. \n",
      "**Caveat: Data dumps only appear only on Sundays, so you can't check on things that happened after Sunday before the next Sunday.**\n",
      "\n",
      "Here are the steps how to use it:\n",
      "\n",
      "1. Exeute the standard data reduction pipeline as instructed [here](Standard reduction pipeline). This will create the HDF5 database files in the same folder where you have saved the CSV database dump file. **Note: This step will create currently approx. 5 GB of data.**\n",
      "2. Go to the `P4_sandbox/planet4` folder.\n",
      "3. Locate the program `gold_standard_checker.py`. When launching it with option `-h` you get some (hopefully) helpful text, but in summary, it has \n",
      "    * one required (i.e. 'positional') argument which is the path to the HTML file you want to check your status for,\n",
      "    * two optional arguments, but for users other than me they become required, unless you store your data in exactly the same paths as me, (which is unlikely), and you want to check for yourself and not me (likely).\n",
      "     1. `--user` controls with username to check for. A list is given with the exact spelling you have to choose from\n",
      "     2. `--datadir` is the path to the directory where you stored the CSV datafile.\n",
      "\n",
      "For example, the command could look like this for Meg:\n",
      "```bash\n",
      "python gold_standard_checker.py html_file_path --username mschwamb --datadir path_to_folder_with_csv\n",
      "```\n",
      "\n",
      "If you find any bugs, don't hesitated to report them with the `Issues` link on the right."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
