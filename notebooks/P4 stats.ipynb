{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Task:-Define-status-of-Planet-4\" data-toc-modified-id=\"Task:-Define-status-of-Planet-4-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Task: Define status of Planet 4</a></span><ul class=\"toc-item\"><li><span><a href=\"#Database-format\" data-toc-modified-id=\"Database-format-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Database format</a></span></li><li><span><a href=\"#Image-IDs\" data-toc-modified-id=\"Image-IDs-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Image IDs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-NaNs\" data-toc-modified-id=\"Cleaning-NaNs-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Cleaning NaNs</a></span></li><li><span><a href=\"#After-NaNs-are-removed\" data-toc-modified-id=\"After-NaNs-are-removed-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>After NaNs are removed</a></span></li></ul></li><li><span><a href=\"#Classification-IDs\" data-toc-modified-id=\"Classification-IDs-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Classification IDs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Uniqueness-within-Image_ID!\" data-toc-modified-id=\"Uniqueness-within-Image_ID!-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Uniqueness within Image_ID!</a></span></li></ul></li><li><span><a href=\"#Percentages-done.\" data-toc-modified-id=\"Percentages-done.-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Percentages done.</a></span></li><li><span><a href=\"#Separate-for-seasons\" data-toc-modified-id=\"Separate-for-seasons-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Separate for seasons</a></span><ul class=\"toc-item\"><li><span><a href=\"#Percentages-done\" data-toc-modified-id=\"Percentages-done-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Percentages done</a></span></li><li><span><a href=\"#MDAP-2014\" data-toc-modified-id=\"MDAP-2014-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>MDAP 2014</a></span></li></ul></li></ul></li><li><span><a href=\"#Problem-??\" data-toc-modified-id=\"Problem-??-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Problem ??</a></span><ul class=\"toc-item\"><li><span><a href=\"#Group-by-user_name-instead-of-classification_id\" data-toc-modified-id=\"Group-by-user_name-instead-of-classification_id-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Group by user_name instead of classification_id</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-subframe-known-as-jp7\" data-toc-modified-id=\"The-subframe-known-as-jp7-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>The subframe known as jp7</a></span></li></ul></li><li><span><a href=\"#Some-instructive-plots\" data-toc-modified-id=\"Some-instructive-plots-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Some instructive plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-over-required-constraint\" data-toc-modified-id=\"Plot-over-required-constraint-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Plot over required constraint</a></span></li><li><span><a href=\"#How-do-the-different-existing-user-counts-distribute\" data-toc-modified-id=\"How-do-the-different-existing-user-counts-distribute-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>How do the different existing user counts distribute</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Define status of Planet 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the `pandas` data table analyis library and check which version I'm using (as I'm constantly changing that to keep up-to-date.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from planet4 import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database format\n",
    "\n",
    "In a different notebook (the document you are looking at is called an [IPython Notebook](http://ipython.org/notebook.html)) I have converted the mongodb database text dump from Planet 4 into \n",
    "[HDF format](http://en.wikipedia.org/wiki/Hierarchical_Data_Format). \n",
    "I saved it in a subformat for very fast read-speed into memory; the 2 GB file currently loads within 20 seconds on my Macbook Pro.\n",
    "\n",
    "By the way, this HDF5 format is supported in IDL and Matlab as well, so I could provide this file as a download for Candy and others, if wanted.\n",
    "\n",
    "I save the object I get back here in the variable *df*, a shortcut for `dataframe`, which is the essential table object of the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(get_data.get_current_database_fname(), 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what did we receive in `df` (note that type 'object' often means `string` in our case, but could mean also a different complex datatype):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"/Users/klay6683/local_data/2018-10-14_planet_four_classifications_queryable_cleaned.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from planet4 import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsids = df.image_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for obsid in tqdm(obsids):\n",
    "    sub_df = df[df.image_name==obsid]\n",
    "    results.append(stats.get_status_per_classifications(sub_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(results, index=obsids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv(\"current_status.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[s<50].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[s<50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat current_status.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 5 rows of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(df.image_name.unique()).to_csv(\"image_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image IDs\n",
    "For a simple first task, let's get a list of unique image ids, to know how many objects have been published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = df.image_id.unique()\n",
    "print img_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might have some NaN values in there, depending on how the database dump was created. Let's check if that's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.image_id.notnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's only True as an answer above, you can skip the nan-cleaning section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.image_id.isnull()].T # .T just to have it printed like a column, not a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one version of the database dump, I had the last row being completely NaN, so I dropped it with the next command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(10718113)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that there's nothing with a NaN image_id now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.image_id.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After NaNs are removed\n",
    "Ok, now we should only get non-NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = df.image_id.unique()\n",
    "img_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how many objects were online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_all = len(img_ids)\n",
    "no_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification IDs\n",
    "Now we need to find out how often each image_id has been looked at. \n",
    "For that we have the groupby functionality. \n",
    "Specifically, because we want to know how many citizens have submitted a classification for each image_id, we need to group by the image_id and count the unique classification_ids within each image_id group. \n",
    "\n",
    "### Uniqueness within Image_ID!\n",
    "We need to constrain for uniqueness because each classified object is included with the same classification_id and we don't want to count them more than once, because we are interested in the overall submission only for now.\n",
    "\n",
    "In other words: Because the different fans, blobs and interesting things for one image_id have all been submitted with the same classification_id, I need to constrain to unique classification_ids, otherwise images with a lot of submitted items would appear 'more completed' just for having a lot of fan-content, and not for being analyzed by a lot of citizens, which is what we want.\n",
    "\n",
    "First, I confirm that classification_ids indeed have more than 1 entry, i.e. when there was more than one object classified by a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df.classification_id, sort=False).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that is the case.\n",
    "Now, group those classification_ids by the image_ids and save the grouping. Switch off sorting for speed, we want to sort by the counts later anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = df.classification_id.groupby(df.image_id, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate each group by finding the size of the unique list of classification_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = grouping.agg(lambda x: x.unique().size)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order the counts by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.order(ascending=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the length of this counts data series is 98220, exactly the number of unique image_ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentages done.\n",
    "\n",
    "By constraining the previous data series for the value it has (the counts) and look at the length of the remaining data, we can determine the status of the finished rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[counts >= 30].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty disappointing, but alas, the cold hard truth. \n",
    "This means, taking all submitted years into account in the data, we have currently only the following percentage done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[counts>= 30].size / float(no_all) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wishing to see higher values, I was for some moments contemplating if one maybe has to sum up the different counts to be correct, but I don't think that's it.\n",
    "\n",
    "The way I see it, one has to decide in what 'phase-space' one works to determine the status of Planet4.\n",
    "Either in the phase space of total subframes or in the total number of classifications. And I believe to determine the finished state of Planet4 it is sufficient and actually easier to focus on the available number of subframes and determine how often each of them has been looked at.\n",
    "\n",
    "## Separate for seasons\n",
    "The different seasons of our south polar observations are separated by several counts of the `thousands` digit in the `image_id` column of the original HiRISE image id, in P4 called image_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from planet4 import helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.define_season_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.unique_image_ids_per_season(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_all = df.season.value_counts()\n",
    "no_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentages done\n",
    "Now I code a short function with the code I used above to create the counts of classification_ids per image_id. Note again the restriction to uniqueness of classification_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_per_classification_id(df, unique=True):\n",
    "    grouping = df.classification_id.groupby(df.image_id, sort=False)\n",
    "    # because I only grouped the classification_id column above, this function is only\n",
    "    # applied to it. First, reduce to a unique list, and then save the size of that list.\n",
    "    if unique:\n",
    "        return grouping.agg(lambda x: x.unique().size)\n",
    "    else:\n",
    "        return grouping.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.image_name.groupby(df.season).agg(lambda x:x.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_all = df.image_id.groupby(df.season).agg(lambda x: x.unique().size)\n",
    "no_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def done_per_season(season, limit, unique=True, in_percent=True):\n",
    "    subdf = df[df.season == season]\n",
    "    counts_per_classid = get_counts_per_classification_id(subdf, unique)\n",
    "    no_done = counts_per_classid[counts_per_classid >= limit].size\n",
    "    if in_percent:\n",
    "        return 100.0 * no_done / no_all[season]\n",
    "    else:\n",
    "        return no_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in [1,2,3]:\n",
    "    print season\n",
    "    print done_per_season(season, 30, in_percent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDAP 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season1 = df.loc[df.season==1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inca = season1.loc[season1.image_name.str.endswith('_0985')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan = season1.loc[season1.image_name.str.endswith('_0935')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.get_status(inca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.get_status(manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.get_status(season1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inca_images = \"\"\"PSP_002380_0985,PSP_002868_0985,PSP_003092_0985,PSP_003158_0985,PSP_003237_0985,PSP_003448_0985,PSP_003593_0985,PSP_003770_0815,PSP_003804_0985,PSP_003928_0815\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inca_images = inca_images.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inca = df.loc[df.image_name.isin(inca_images),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.get_status(inca, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in inca_images:\n",
    "    print img\n",
    "    print hf.get_status(season1.loc[season1.image_name == img,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneimage = season1.loc[season1.image_name == 'PSP_003928_0815',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = oneimage.image_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = hf.classification_counts_per_image(season1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[img_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = []\n",
    "for img_id in img_ids:\n",
    "    container.append(counts[img_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(container)\n",
    "savefig('done_for_PSP_003928_0815.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = hf.classification_counts_per_image(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[counts >=30].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code I not only check for the different years, but also the influence on the demanded limit of counts to define a subframe as 'finished'.\n",
    "\n",
    "To collect the data I create an empty dataframe with an index ranging through the different limits I want to check (i.e. `range(30,101,10)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import OrderedDict\n",
    "results = pd.DataFrame(index=range(30,101,10))\n",
    "for season in [1,2,3]:\n",
    "    print season\n",
    "    sys.stdout.flush() # to force a print out of the std buffer\n",
    "    subdf = df[df.season == season]\n",
    "    counts = get_counts_per_classification_id(subdf)\n",
    "    values = OrderedDict()\n",
    "    for limit in results.index:\n",
    "        values[limit] = done_per_season(season, limit)\n",
    "    results[season] = values.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem ??\n",
    "## Group by user_name instead of classification_id\n",
    "\n",
    "I realised that user_ids should provide just the same access to the performed counts, because each classification_id should have exactly one user_id, as they are created when that user clicks on *Submit*, right? \n",
    "At least that's how I understood it.\n",
    "\n",
    "So imagine my surprise when I found out it isn't the same answer. And unfortunately it looks like we have to reduce our dataset even further by apparent multiple submissions of the same classification, but let's see.\n",
    "\n",
    "First, create the respective function to determine counts via the user_name instead of classification_id after grouping for image_id.\n",
    "This first grouping by image_id is the essential step for the determination how often a particular image_id has been worked on, so that doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_per_user_name(df):\n",
    "    grouping = df.user_name.groupby(df.image_id, sort=False)\n",
    "    counts = grouping.agg(lambda x: x.unique().size)\n",
    "#    counts = counts.order(ascending=False)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_user = get_counts_per_user_name(df)\n",
    "counts_by_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that again to the output for classifying per classification_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_class = get_counts_per_classification_id(df)\n",
    "counts_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, _not_ the same result! Let's dig deeper.\n",
    "\n",
    "### The subframe known as jp7\n",
    "Focus on one image_id and study what is happening there. I first get a sub-table for the subframe 'jp7' and determine the user_names that worked on that subframe.\n",
    "\n",
    "Then I loop over the names, filtering another sub-part of the table where the current user worked on jp7. \n",
    "According to the hypothesis that a classification_id is created for a user at submisssion time and the idea that a user should not see an image twice, there should only be one classification_id in that sub-part.\n",
    "\n",
    "I am testing that by checking if the unique list of classification_ids has a length $>1$. If it does, I print out the user_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp7 = df[df.image_id == 'APF0000jp7']\n",
    "unique_users = jp7.user_name.unique()\n",
    "# having the list of users that worked on jp7\n",
    "for user in unique_users:\n",
    "    subdf = jp7[jp7.user_name == user]\n",
    "    if len(subdf.classification_id.unique()) > 1:\n",
    "        print user, len(subdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let's have a look at the data for the first user_name for the subframe jp7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp7[jp7.user_name == 'not-logged-in-8d495c463aeffd67c08b2dfc1141f33b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note that the creation time of these 2 different classifications is different, so it looks like this user has seen the jp7 subframe more than once.\n",
    "\n",
    "But then when you scroll this html table to the right, you will notice that the submitted object has the exact same coordinates in both classifications? \n",
    "How likely is it, that the user finds the exact same coordinates in less than 60 seconds?\n",
    "\n",
    "So the question is, is this really a new classification and the user has done it twice? Or was the same thing submitted twice? Hopefully Meg knows the answer to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some instructive plots\n",
    "\n",
    "### Plot over required constraint\n",
    "\n",
    "I found it instructive to look at how the status of finished data depends on the limit we put on the reached counts per image_id (i.e. subframe).\n",
    "\n",
    "Also, how does it change when looking for unique user_names per image_id instead of unique classification_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[[2,3]].plot()\n",
    "xlabel('Required number of analyses submitted to be considered \"done\".')\n",
    "ylabel('Current percentage of dataset finished [%]')\n",
    "title(\"Season 2 and 3 status, depending on definition of 'done'.\")\n",
    "savefig('Season2_3_status.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1,101)\n",
    "per_class = []\n",
    "per_user = []\n",
    "for val in x:\n",
    "    per_class.append(100 * counts_by_class[counts_by_class >= val].size/float(no_all))\n",
    "    per_user.append(100 * counts_by_user[counts_by_user >= val].size/float(no_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x,per_class)\n",
    "plot(x, per_user)\n",
    "xlabel('Counts constraint for _finished_ criterium')\n",
    "ylabel('Current percent finished [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so not that big a deal until we require more than 80 classifications to be done.\n",
    "\n",
    "### How do the different existing user counts distribute\n",
    "\n",
    "The method 'value_counts()' basically delivers a histogram on the counts_by_user data series.\n",
    "In other words, it shows how the frequency of classifications distribute over the dataset. It shows an to be expected peak close to 100, because that's what we are aiming now and the system does _today_ not anymore show a subframe that has been seen 100 times.\n",
    "\n",
    "But it also shows quite some _waste_ in citizen power from all the counts that went for counts > 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_user.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_user.value_counts().plot(style='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_work = df.classification_id.groupby(df.user_name).agg(lambda x: x.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_work.order(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.user_name=='gwyneth walker'].classification_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions as hf\n",
    "reload(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.classification_counts_for_user('Kitharode', df).hist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.classification_counts_for_user('Paul Johnson', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(df.marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
