{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Define status of Planet 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the `pandas` data table analyis library and check which version I'm using (as I'm constantly changing that to keep up-to-date.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import planet4 as p4\n",
    "import pandas as pd\n",
    "from planet4 import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = io.DBManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf(db_fname, 'df', stop=1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image IDs\n",
    "For a simple first task, let's get a list of unique image ids, to know how many objects have been published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_ids = pd.read_hdf(db_fname, 'df', columns=['image_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_ids = df.image_id.unique()\n",
    "print img_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how many objects were online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_all = len(img_ids)\n",
    "no_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification IDs\n",
    "Now we need to find out how often each image_id has been looked at. \n",
    "For that we have the groupby functionality. \n",
    "Specifically, because we want to know how many citizens have submitted a classification for each image_id, we need to group by the image_id and count the unique classification_ids within each image_id group. \n",
    "\n",
    "### Uniqueness within Image_ID!\n",
    "We need to constrain for uniqueness because each classified object is included with the same classification_id and we don't want to count them more than once, because we are interested in the overall submission only for now.\n",
    "\n",
    "In other words: Because the different fans, blobs and interesting things for one image_id have all been submitted with the same classification_id, I need to constrain to unique classification_ids, otherwise images with a lot of submitted items would appear 'more completed' just for having a lot of fan-content, and not for being analyzed by a lot of citizens, which is what we want.\n",
    "\n",
    "First, I confirm that classification_ids indeed have more than 1 entry, i.e. when there was more than one object classified by a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby(df.classification_id, sort=False).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that is the case.\n",
    "Now, group those classification_ids by the image_ids and save the grouping. Switch off sorting for speed, we want to sort by the counts later anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouping = df.classification_id.groupby(df.image_id, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate each group by finding the size of the unique list of classification_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = grouping.agg(lambda x: x.unique().size)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order the counts by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = counts.order(ascending=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the length of this counts data series is 98096, exactly the number of unique image_ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentages done.\n",
    "\n",
    "By constraining the previous data series for the value it has (the counts) and look at the length of the remaining data, we can determine the status of the finished rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts[counts >= 30].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty disappointing, but alas, the cold hard truth. \n",
    "This means, taking all submitted years into account in the data, we have currently only the following percentage done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts[counts>= 30].size / float(no_all) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wishing to see higher values, I was for some moments contemplating if one maybe has to sum up the different counts to be correct, but I don't think that's it.\n",
    "\n",
    "The way I see it, one has to decide in what 'phase-space' one works to determine the status of Planet4.\n",
    "Either in the phase space of total subframes or in the total number of classifications. And I believe to determine the finished state of Planet4 it is sufficient and actually easier to focus on the available number of subframes and determine how often each of them has been looked at.\n",
    "\n",
    "## Separate for seasons\n",
    "The different seasons of our south polar observations are separated by several counts of the `thousands` digit in the `image_id` column of the original HiRISE image id, in P4 called image_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# str[5:7] is the 2-digit thousands count in, e.g., ESP_011234_0950, in this case 11.\n",
    "df['thousands'] = df.image_name.str[5:7].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thousands = df.thousands.value_counts().sort_index()\n",
    "thousands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, we have groups of [1..5, 11..13, 20..22].\n",
    "Let's add another season column to the dataframe, first filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['season'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first season, we actually don't need to look at the thousands counter, as the first 3 letters of the image_names started all with PSP in the first season (for 'P_rimary S_cience P_hase').\n",
    "Now let's set all rows with names starting with 'PSP' to season 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'season'][df.image_name.str.startswith('PSP')] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the later seasons, we actually need to group by the thousands counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'season'][(df.thousands > 10) & (df.thousands < 20)] = 2\n",
    "df.loc[:, 'season'][df.thousands > 19] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for all seasons, how many rows to we have in the overall data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_all = df.season.value_counts()\n",
    "no_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentages done\n",
    "Now I code a short function with the code I used above to create the counts of classification_ids per image_id. Note again the restriction to uniqueness of classification_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_counts_per_classification_id(df, unique=True):\n",
    "    grouping = df.classification_id.groupby(df.image_id, sort=False)\n",
    "    # because I only grouped the classification_id column above, this function is only\n",
    "    # applied to it. First, reduce to a unique list, and then save the size of that list.\n",
    "    if unique:\n",
    "        return grouping.agg(lambda x: x.unique().size)\n",
    "    else:\n",
    "        return grouping.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.image_name.groupby(df.season).agg(lambda x:x.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_all = df.image_id.groupby(df.season).agg(lambda x: x.unique().size)\n",
    "no_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def done_per_season(season, limit, unique=True, in_percent=True):\n",
    "    subdf = df[df.season == season]\n",
    "    counts_per_classid = get_counts_per_classification_id(subdf, unique)\n",
    "    no_done = counts_per_classid[counts_per_classid >= limit].size\n",
    "    if in_percent:\n",
    "        return 100.0 * no_done / no_all[season]\n",
    "    else:\n",
    "        return no_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for season in [1,2,3]:\n",
    "    print season\n",
    "    print done_per_season(season, 30, in_percent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code I not only check for the different years, but also the influence on the demanded limit of counts to define a subframe as 'finished'.\n",
    "\n",
    "To collect the data I create an empty dataframe with an index ranging through the different limits I want to check (i.e. `range(30,101,10)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import OrderedDict\n",
    "results = pd.DataFrame(index=range(30,101,10))\n",
    "for season in [1,2,3]:\n",
    "    print season\n",
    "    sys.stdout.flush() # to force a print out of the std buffer\n",
    "    subdf = df[df.season == season]\n",
    "    counts = get_counts_per_classification_id(subdf)\n",
    "    values = OrderedDict()\n",
    "    for limit in results.index:\n",
    "        values[limit] = done_per_season(season, limit)\n",
    "    results[season] = values.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.round(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem ??\n",
    "## Group by user_name instead of classification_id\n",
    "\n",
    "I realised that user_ids should provide just the same access to the performed counts, because each classification_id should have exactly one user_id, as they are created when that user clicks on *Submit*, right? \n",
    "At least that's how I understood it.\n",
    "\n",
    "So imagine my surprise when I found out it isn't the same answer. And unfortunately it looks like we have to reduce our dataset even further by apparent multiple submissions of the same classification, but let's see.\n",
    "\n",
    "First, create the respective function to determine counts via the user_name instead of classification_id after grouping for image_id.\n",
    "This first grouping by image_id is the essential step for the determination how often a particular image_id has been worked on, so that doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_counts_per_user_name(df):\n",
    "    grouping = df.user_name.groupby(df.image_id, sort=False)\n",
    "    counts = grouping.agg(lambda x: x.unique().size)\n",
    "#    counts = counts.order(ascending=False)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_user = get_counts_per_user_name(df)\n",
    "counts_by_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that again to the output for classifying per classification_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_class = get_counts_per_classification_id(df)\n",
    "counts_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, _not_ the same result! Let's dig deeper.\n",
    "\n",
    "### The subframe known as jp7\n",
    "Focus on one image_id and study what is happening there. I first get a sub-table for the subframe 'jp7' and determine the user_names that worked on that subframe.\n",
    "\n",
    "Then I loop over the names, filtering another sub-part of the table where the current user worked on jp7. \n",
    "According to the hypothesis that a classification_id is created for a user at submisssion time and the idea that a user should not see an image twice, there should only be one classification_id in that sub-part.\n",
    "\n",
    "I am testing that by checking if the unique list of classification_ids has a length $>1$. If it does, I print out the user_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jp7 = df[df.image_id == 'APF0000jp7']\n",
    "unique_users = jp7.user_name.unique()\n",
    "# having the list of users that worked on jp7\n",
    "for user in unique_users:\n",
    "    subdf = jp7[jp7.user_name == user]\n",
    "    if len(subdf.classification_id.unique()) > 1:\n",
    "        print user, len(subdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let's have a look at the data for the first user_name for the subframe jp7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jp7[jp7.user_name == 'not-logged-in-8d495c463aeffd67c08b2dfc1141f33b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note that the creation time of these 2 different classifications is different, so it looks like this user has seen the jp7 subframe more than once.\n",
    "\n",
    "But then when you scroll this html table to the right, you will notice that the submitted object has the exact same coordinates in both classifications? \n",
    "How likely is it, that the user finds the exact same coordinates in less than 60 seconds?\n",
    "\n",
    "So the question is, is this really a new classification and the user has done it twice? Or was the same thing submitted twice? Hopefully Meg knows the answer to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some instructive plots\n",
    "\n",
    "### Plot over required constraint\n",
    "\n",
    "I found it instructive to look at how the status of finished data depends on the limit we put on the reached counts per image_id (i.e. subframe).\n",
    "\n",
    "Also, how does it change when looking for unique user_names per image_id instead of unique classification_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results[[2,3]].plot()\n",
    "xlabel('Required number of analyses submitted to be considered \"done\".')\n",
    "ylabel('Current percentage of dataset finished [%]')\n",
    "title(\"Season 2 and 3 status, depending on definition of 'done'.\")\n",
    "savefig('Season2_3_status.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = range(1,101)\n",
    "per_class = []\n",
    "per_user = []\n",
    "for val in x:\n",
    "    per_class.append(100 * counts_by_class[counts_by_class >= val].size/float(no_all))\n",
    "    per_user.append(100 * counts_by_user[counts_by_user >= val].size/float(no_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(x,per_class)\n",
    "plot(x, per_user)\n",
    "xlabel('Counts constraint for _finished_ criterium')\n",
    "ylabel('Current percent finished [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so not that big a deal until we require more than 80 classifications to be done.\n",
    "\n",
    "### How do the different existing user counts distribute\n",
    "\n",
    "The method 'value_counts()' basically delivers a histogram on the counts_by_user data series.\n",
    "In other words, it shows how the frequency of classifications distribute over the dataset. It shows an to be expected peak close to 100, because that's what we are aiming now and the system does _today_ not anymore show a subframe that has been seen 100 times.\n",
    "\n",
    "But it also shows quite some _waste_ in citizen power from all the counts that went for counts > 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_user.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_user.value_counts().plot(style='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_work = df.classification_id.groupby(df.user_name).agg(lambda x: x.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_work.order(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df.user_name=='gwyneth walker'].classification_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helper_functions as hf\n",
    "reload(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hf.classification_counts_for_user('Kitharode', df).hist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hf.classification_counts_for_user('Paul Johnson', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(df.marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 'INVESTIGATION OF POLAR SEASONAL FAN DEPOSITS USING CROWDSOURCING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:stable]",
   "language": "python",
   "name": "conda-env-stable-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
